{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNuLJL0XHVB/UEmO+BdTE/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manushi0304/Diabetic_Retinopathy/blob/main/HybridEfficientNetHYbrid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdFBqIyYKSdr",
        "outputId": "8d635d68-54bd-4b8b-8f13-8a3b4e653a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "=== EfficientNetV2B0 paths ===\n",
            "EFFICIENTNET_KERAS_PATH  : /content/drive/MyDrive/DiabeticProject/saved_models/EfficientNetV2B0_single_split.keras exists: True\n",
            "EFFICIENTNET_TFLITE_FP16 : /content/drive/MyDrive/DiabeticProject/tflite/EfficientNetV2B0_model_fp16.tflite exists: True\n",
            "HYBRID_SAVE_DIR          : /content/drive/MyDrive/DiabeticProject/hybrid_models\n"
          ]
        }
      ],
      "source": [
        "# ===== Cell 1: Mount Drive & locate EfficientNetV2B0 =====\n",
        "import os, fnmatch, datetime\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# You usually don't need to clear the mountpoint, but keeping your logic:\n",
        "mountpoint = '/content/drive'\n",
        "if os.path.exists(mountpoint):\n",
        "    print(f\"Clearing existing mountpoint: {mountpoint}\")\n",
        "    try:\n",
        "        for item in os.listdir(mountpoint):\n",
        "            item_path = os.path.join(mountpoint, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "            else:\n",
        "                os.remove(item_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error clearing mountpoint: {e}\")\n",
        "\n",
        "drive.mount(mountpoint, force_remount=True)\n",
        "\n",
        "BASE   = f\"{mountpoint}/MyDrive/DiabeticProject\"\n",
        "SAVED  = f\"{BASE}/saved_models\"\n",
        "TFLITE = f\"{BASE}/tflite\"\n",
        "HYBRID_SAVE_DIR = f\"{BASE}/hybrid_models\"\n",
        "os.makedirs(HYBRID_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "def find_one(root, patterns=(\".keras\", \".h5\"), name_contains=(\"efficientnetv2b0\", \"efficientnet\", \"v2b0\")):\n",
        "    hits = []\n",
        "    for r, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if not f.lower().endswith(patterns):\n",
        "                continue\n",
        "            if any(k in f.lower() for k in name_contains):\n",
        "                hits.append(os.path.join(r, f))\n",
        "    hits.sort(key=lambda p:(-os.path.getmtime(p), len(p)))\n",
        "    return hits[0] if hits else None\n",
        "\n",
        "def find_tflite(root, name_contains=(\"efficientnetv2b0\", \"efficientnet\", \"v2b0\"), quant=\"fp16\"):\n",
        "    hits = []\n",
        "    for r, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if not f.lower().endswith(\".tflite\"):\n",
        "                continue\n",
        "            if any(k in f.lower() for k in name_contains) and quant in f.lower():\n",
        "                hits.append(os.path.join(r, f))\n",
        "    hits.sort(key=lambda p:(-os.path.getmtime(p), len(p)))\n",
        "    return hits[0] if hits else None\n",
        "\n",
        "# Pick EfficientNet files (from your listing)\n",
        "EFFICIENTNET_KERAS_PATH  = find_one(SAVED, (\".keras\", \".h5\"), (\"efficientnetv2b0\", \"efficientnet\", \"v2b0\"))\n",
        "EFFICIENTNET_TFLITE_FP16 = find_tflite(TFLITE, (\"efficientnetv2b0\", \"efficientnet\", \"v2b0\"), \"fp16\")\n",
        "\n",
        "# Fallback to your exact known file names if discovery missed:\n",
        "if not EFFICIENTNET_KERAS_PATH:\n",
        "    cand = f\"{SAVED}/EfficientNetV2B0_single_split.keras\"\n",
        "    if os.path.exists(cand): EFFICIENTNET_KERAS_PATH = cand\n",
        "if not EFFICIENTNET_TFLITE_FP16:\n",
        "    cand = f\"{TFLITE}/EfficientNetV2B0_model_fp16.tflite\"\n",
        "    if os.path.exists(cand): EFFICIENTNET_TFLITE_FP16 = cand\n",
        "\n",
        "print(\"=== EfficientNetV2B0 paths ===\")\n",
        "print(\"EFFICIENTNET_KERAS_PATH  :\", EFFICIENTNET_KERAS_PATH,  \"exists:\", os.path.exists(EFFICIENTNET_KERAS_PATH) if EFFICIENTNET_KERAS_PATH else None)\n",
        "print(\"EFFICIENTNET_TFLITE_FP16 :\", EFFICIENTNET_TFLITE_FP16, \"exists:\", os.path.exists(EFFICIENTNET_TFLITE_FP16) if EFFICIENTNET_TFLITE_FP16 else None)\n",
        "print(\"HYBRID_SAVE_DIR          :\", HYBRID_SAVE_DIR)\n",
        "\n",
        "if not EFFICIENTNET_KERAS_PATH:\n",
        "    raise SystemExit(\"‚ùå EfficientNetV2B0 Keras model not found under saved_models.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 2: Locate APTOS Binary via KaggleHub & build dataframe =====\n",
        "import os, re, zipfile, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "try:\n",
        "    _ = subhajeetdas_aptos_2019_jpg_path\n",
        "except NameError:\n",
        "    import kagglehub\n",
        "    subhajeetdas_aptos_2019_jpg_path = kagglehub.dataset_download('subhajeetdas/aptos-2019-jpg')\n",
        "\n",
        "print(\"KaggleHub root:\", subhajeetdas_aptos_2019_jpg_path)\n",
        "\n",
        "def _norm(s): return re.sub(r'[_\\s]+', ' ', s.strip().lower())\n",
        "def _has_binary(dirpath):\n",
        "    try:\n",
        "        kids = [_norm(d) for d in os.listdir(dirpath) if os.path.isdir(os.path.join(dirpath, d))]\n",
        "        return ('dr' in kids) and ('no dr' in kids)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _maybe_unzip_here(root):\n",
        "    for f in os.listdir(root):\n",
        "        if f.lower().endswith(\".zip\") and \"aptos\" in f.lower() and \"binary\" in f.lower():\n",
        "            z = os.path.join(root, f)\n",
        "            out = os.path.join(root, os.path.splitext(f)[0])\n",
        "            if not os.path.isdir(out):\n",
        "                print(\"üì¶ Extracting:\", z)\n",
        "                with zipfile.ZipFile(z, 'r') as zf: zf.extractall(out)\n",
        "\n",
        "def find_binary_root():\n",
        "    _maybe_unzip_here(subhajeetdas_aptos_2019_jpg_path)\n",
        "    cands = [\n",
        "        os.path.join(subhajeetdas_aptos_2019_jpg_path, \"APTOS 2019 (Original) (Binary)\"),\n",
        "        subhajeetdas_aptos_2019_jpg_path,\n",
        "    ]\n",
        "    for c in cands:\n",
        "        if os.path.isdir(c) and _has_binary(c): return c\n",
        "    for dirpath, _, files in os.walk(subhajeetdas_aptos_2019_jpg_path):\n",
        "        if _has_binary(dirpath): return dirpath\n",
        "        if any(f.lower().endswith(\".zip\") for f in files): _maybe_unzip_here(dirpath)\n",
        "    for c in [\n",
        "        \"/content/drive/MyDrive/DiabeticProject/APTOS 2019 (Original) (Binary)\",\n",
        "        \"/content/drive/MyDrive/APTOS 2019 (Original) (Binary)\"\n",
        "    ]:\n",
        "        if os.path.isdir(c) and _has_binary(c): return c\n",
        "    raise SystemExit(\"‚ùå Could not locate the APTOS Binary dataset.\")\n",
        "\n",
        "base_path = find_binary_root()\n",
        "print(\"‚úÖ Using dataset root:\", base_path)\n",
        "print(\"Class folders:\", os.listdir(base_path))\n",
        "\n",
        "exts = ('.png','.jpg','.jpeg','.bmp','.tif','.tiff','.webp')\n",
        "rows = []\n",
        "for cls in os.listdir(base_path):\n",
        "    d = os.path.join(base_path, cls)\n",
        "    if not os.path.isdir(d): continue\n",
        "    lab = 1 if _norm(cls)=='dr' else 0 if _norm(cls)=='no dr' else None\n",
        "    if lab is None: continue\n",
        "    for f in os.listdir(d):\n",
        "        if f.lower().endswith(exts):\n",
        "            rows.append((os.path.join(d, f), lab))\n",
        "df = pd.DataFrame(rows, columns=['image_path','label'])\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.30, stratify=df['label'], random_state=42)\n",
        "valid_df, test_df = train_test_split(temp_df, test_size=0.50, stratify=temp_df['label'], random_state=42)\n",
        "for name, d in [(\"Train\",train_df),(\"Valid\",valid_df),(\"Test\",test_df)]:\n",
        "    print(f\"{name}: total={len(d)} | No DR={(d['label']==0).sum()} | DR={(d['label']==1).sum()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqfqxZs4K7_t",
        "outputId": "9e9cb216-453b-4dca-ef8a-150c9229b6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/subhajeetdas/aptos-2019-jpg?dataset_version_number=12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.82G/2.82G [00:33<00:00, 89.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KaggleHub root: /root/.cache/kagglehub/datasets/subhajeetdas/aptos-2019-jpg/versions/12\n",
            "‚úÖ Using dataset root: /root/.cache/kagglehub/datasets/subhajeetdas/aptos-2019-jpg/versions/12/APTOS 2019 (Original) (Binary)\n",
            "Class folders: ['Details.txt', 'No DR', 'DR']\n",
            "Train: total=2563 | No DR=1263 | DR=1300\n",
            "Valid: total=549 | No DR=271 | DR=278\n",
            "Test: total=550 | No DR=271 | DR=279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 3: Deduplicate + grouped split (no leakage) =====\n",
        "import os, re, hashlib, numpy as np, pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "pool = pd.concat([train_df, valid_df, test_df], ignore_index=True).drop_duplicates(\"image_path\")\n",
        "pool[\"label\"] = pool[\"label\"].astype(int)\n",
        "\n",
        "def sha1(fp, chunk=1<<20):\n",
        "    h = hashlib.sha1()\n",
        "    with open(fp, 'rb') as f:\n",
        "        while True:\n",
        "            b = f.read(chunk)\n",
        "            if not b: break\n",
        "            h.update(b)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def ahash(path, n=8):\n",
        "    try:\n",
        "        img = Image.open(path).convert('L').resize((n,n))\n",
        "    except Exception:\n",
        "        return None\n",
        "    arr = np.asarray(img, dtype=np.float32)\n",
        "    bits = (arr > arr.mean()).astype(np.uint8).ravel()\n",
        "    out = []\n",
        "    for i in range(0, bits.size, 8):\n",
        "        byte = 0\n",
        "        for j in range(8):\n",
        "            if i+j < bits.size and bits[i+j]: byte |= (1 << (7-j))\n",
        "        out.append(byte)\n",
        "    return tuple(out)\n",
        "\n",
        "print(\"üîç SHA1 ‚Ä¶\")\n",
        "pool[\"sha1\"] = pool[\"image_path\"].apply(sha1)\n",
        "pool = pool.drop_duplicates(\"sha1\").reset_index(drop=True)\n",
        "\n",
        "print(\"üîç aHash ‚Ä¶\")\n",
        "pool[\"ahash\"] = pool[\"image_path\"].apply(ahash)\n",
        "pool = pool[pool[\"ahash\"].notna()].drop_duplicates(\"ahash\").reset_index(drop=True)\n",
        "\n",
        "def pid_from_path(p):\n",
        "    stem = os.path.splitext(os.path.basename(p))[0]\n",
        "    return re.sub(r'_(left|right)$', '', stem, flags=re.IGNORECASE)\n",
        "\n",
        "pool[\"group\"] = pool[\"image_path\"].apply(pid_from_path)\n",
        "\n",
        "gss = GroupShuffleSplit(n_splits=1, train_size=0.70, random_state=42)\n",
        "i_tr, i_tmp = next(gss.split(pool, groups=pool[\"group\"]))\n",
        "df_tr, df_tmp = pool.iloc[i_tr].reset_index(drop=True), pool.iloc[i_tmp].reset_index(drop=True)\n",
        "\n",
        "gss2 = GroupShuffleSplit(n_splits=1, train_size=0.50, random_state=42)\n",
        "i_va, i_te = next(gss2.split(df_tmp, groups=df_tmp[\"group\"]))\n",
        "df_va, df_te = df_tmp.iloc[i_va].reset_index(drop=True), df_tmp.iloc[i_te].reset_index(drop=True)\n",
        "\n",
        "def _cnt(tag, d):\n",
        "    print(f\"{tag}: total={len(d)} | No DR={(d['label']==0).sum()} | DR={(d['label']==1).sum()} | groups={d['group'].nunique()}\")\n",
        "print(\"\\n‚úÖ Post-dedup grouped split:\")\n",
        "_cnt(\"Train\", df_tr); _cnt(\"Valid\", df_va); _cnt(\"Test \", df_te)\n",
        "print(\"Group overlaps:\",\n",
        "      len(set(df_tr.group)&set(df_va.group)),\n",
        "      len(set(df_tr.group)&set(df_te.group)),\n",
        "      len(set(df_va.group)&set(df_te.group)))\n",
        "\n",
        "train_df, valid_df, test_df = df_tr.drop(columns=[\"sha1\",\"ahash\"]), df_va.drop(columns=[\"sha1\",\"ahash\"]), df_te.drop(columns=[\"sha1\",\"ahash\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1HVTOv3KudZ",
        "outputId": "006efcfc-a533-4bce-91d1-8734ecef44eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç SHA1 ‚Ä¶\n",
            "üîç aHash ‚Ä¶\n",
            "\n",
            "‚úÖ Post-dedup grouped split:\n",
            "Train: total=1242 | No DR=528 | DR=714 | groups=1213\n",
            "Valid: total=265 | No DR=121 | DR=144 | groups=260\n",
            "Test : total=265 | No DR=116 | DR=149 | groups=260\n",
            "Group overlaps: 0 0 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 3: Deduplicate + grouped split (no leakage) ‚Äî works for EfficientNetV2B0 too =====\n",
        "import os, re, hashlib, numpy as np, pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "pool = pd.concat([train_df, valid_df, test_df], ignore_index=True).drop_duplicates(\"image_path\")\n",
        "pool[\"label\"] = pool[\"label\"].astype(int)\n",
        "\n",
        "def sha1(fp, chunk=1<<20):\n",
        "    h = hashlib.sha1()\n",
        "    with open(fp, 'rb') as f:\n",
        "        while True:\n",
        "            b = f.read(chunk)\n",
        "            if not b: break\n",
        "            h.update(b)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def ahash(path, n=8):\n",
        "    try:\n",
        "        img = Image.open(path).convert('L').resize((n,n))\n",
        "    except Exception:\n",
        "        return None\n",
        "    arr = np.asarray(img, dtype=np.float32)\n",
        "    bits = (arr > arr.mean()).astype(np.uint8).ravel()\n",
        "    out = []\n",
        "    for i in range(0, bits.size, 8):\n",
        "        byte = 0\n",
        "        for j in range(8):\n",
        "            if i+j < bits.size and bits[i+j]: byte |= (1 << (7-j))\n",
        "        out.append(byte)\n",
        "    return tuple(out)\n",
        "\n",
        "print(\"üîç SHA1 ‚Ä¶\")\n",
        "pool[\"sha1\"] = pool[\"image_path\"].apply(sha1)\n",
        "pool = pool.drop_duplicates(\"sha1\").reset_index(drop=True)\n",
        "\n",
        "print(\"üîç aHash ‚Ä¶\")\n",
        "pool[\"ahash\"] = pool[\"image_path\"].apply(ahash)\n",
        "pool = pool[pool[\"ahash\"].notna()].drop_duplicates(\"ahash\").reset_index(drop=True)\n",
        "\n",
        "def pid_from_path(p):\n",
        "    stem = os.path.splitext(os.path.basename(p))[0]\n",
        "    return re.sub(r'_(left|right)$', '', stem, flags=re.IGNORECASE)\n",
        "\n",
        "pool[\"group\"] = pool[\"image_path\"].apply(pid_from_path)\n",
        "\n",
        "gss = GroupShuffleSplit(n_splits=1, train_size=0.70, random_state=42)\n",
        "i_tr, i_tmp = next(gss.split(pool, groups=pool[\"group\"]))\n",
        "df_tr, df_tmp = pool.iloc[i_tr].reset_index(drop=True), pool.iloc[i_tmp].reset_index(drop=True)\n",
        "\n",
        "gss2 = GroupShuffleSplit(n_splits=1, train_size=0.50, random_state=42)\n",
        "i_va, i_te = next(gss2.split(df_tmp, groups=df_tmp[\"group\"]))\n",
        "df_va, df_te = df_tmp.iloc[i_va].reset_index(drop=True), df_tmp.iloc[i_te].reset_index(drop=True)\n",
        "\n",
        "def _cnt(tag, d):\n",
        "    print(f\"{tag}: total={len(d)} | No DR={(d['label']==0).sum()} | DR={(d['label']==1).sum()} | groups={d['group'].nunique()}\")\n",
        "print(\"\\n‚úÖ Post-dedup grouped split:\")\n",
        "_cnt(\"Train\", df_tr); _cnt(\"Valid\", df_va); _cnt(\"Test \", df_te)\n",
        "print(\"Group overlaps:\",\n",
        "      len(set(df_tr.group)&set(df_va.group)),\n",
        "      len(set(df_tr.group)&set(df_te.group)),\n",
        "      len(set(df_va.group)&set(df_te.group)))\n",
        "\n",
        "train_df, valid_df, test_df = df_tr.drop(columns=[\"sha1\",\"ahash\"]), df_va.drop(columns=[\"sha1\",\"ahash\"]), df_te.drop(columns=[\"sha1\",\"ahash\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoOhf89zngOk",
        "outputId": "c3d57ad7-8bd3-440a-ceb1-47bcf5333ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç SHA1 ‚Ä¶\n",
            "üîç aHash ‚Ä¶\n",
            "\n",
            "‚úÖ Post-dedup grouped split:\n",
            "Train: total=1242 | No DR=528 | DR=714 | groups=1213\n",
            "Valid: total=265 | No DR=121 | DR=144 | groups=260\n",
            "Test : total=265 | No DR=116 | DR=149 | groups=260\n",
            "Group overlaps: 0 0 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 4 (TFLite-FIRST, EfficientNetV2B0): extract features by tapping pooled tensor (1280-d) =====\n",
        "import os, numpy as np, tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "# EXPECTED GLOBALS: EFFICIENTNET_TFLITE_FP16, train_df, valid_df, test_df\n",
        "assert EFFICIENTNET_TFLITE_FP16 and os.path.exists(EFFICIENTNET_TFLITE_FP16), \\\n",
        "    f\"Full fp16 TFLite not found: {EFFICIENTNET_TFLITE_FP16}\"\n",
        "\n",
        "def _load_tflite(path):\n",
        "    inter = tf.lite.Interpreter(model_path=path, num_threads=os.cpu_count() or 1)\n",
        "    inter.allocate_tensors()\n",
        "    return inter\n",
        "\n",
        "def _prep(img_path, W, H, expect_dtype):\n",
        "    img = Image.open(img_path).convert(\"RGB\").resize((W, H), Image.BILINEAR)\n",
        "    x = np.asarray(img, dtype=np.float32) / 255.0\n",
        "    x = np.expand_dims(x, 0)\n",
        "    if expect_dtype == np.float16: x = x.astype(np.float16)\n",
        "    elif expect_dtype == np.float32: x = x.astype(np.float32)\n",
        "    else: x = x.astype(expect_dtype)\n",
        "    return x\n",
        "\n",
        "def _find_feature_index(inter, preferred_dims=(1280,)):\n",
        "    details = inter.get_tensor_details()\n",
        "    cand = None\n",
        "    # name-based hints first\n",
        "    for d in details:\n",
        "        name = d[\"name\"].decode() if isinstance(d[\"name\"], bytes) else d[\"name\"]\n",
        "        shp = tuple(int(s) for s in (d.get(\"shape_signature\", d[\"shape\"])))\n",
        "        if len(shp) in (1,2) and any(k in name.lower() for k in\n",
        "                                     (\"global_average_pool\",\"global_max_pool\",\"avg_pool\",\"mean\",\"gap\",\"pool\")):\n",
        "            cand = d\n",
        "    # dim-based fallback (EfficientNetV2B0 penultimate = 1280)\n",
        "    if cand is None:\n",
        "        for d in details:\n",
        "            shp = tuple(int(s) for s in (d.get(\"shape_signature\", d[\"shape\"])))\n",
        "            if len(shp) in (1,2) and shp[-1] in preferred_dims:\n",
        "                cand = d\n",
        "                break\n",
        "    if cand is not None:\n",
        "        print(f\"üîé Using feature tensor idx={cand['index']} name={cand.get('name')} shape={cand.get('shape')}\")\n",
        "        return cand[\"index\"]\n",
        "    print(\"‚ö†Ô∏è No internal pool tensor matched ‚Äî falling back to model output.\")\n",
        "    return inter.get_output_details()[0][\"index\"]\n",
        "\n",
        "def extract_features_from_full_tflite(tflite_path, df):\n",
        "    inter = _load_tflite(tflite_path)\n",
        "    inp = inter.get_input_details()[0]\n",
        "    H, W, C = int(inp[\"shape\"][1]), int(inp[\"shape\"][2]), int(inp[\"shape\"][3])\n",
        "    feat_idx = _find_feature_index(inter, preferred_dims=(1280,))\n",
        "    X, y = [], df[\"label\"].astype(int).to_numpy()\n",
        "    for p in df[\"image_path\"]:\n",
        "        a = _prep(p, W, H, inp[\"dtype\"])\n",
        "        inter.set_tensor(inp[\"index\"], a)\n",
        "        inter.invoke()\n",
        "        f = inter.get_tensor(feat_idx).reshape(-1)\n",
        "        X.append(f)\n",
        "    X = np.vstack(X)\n",
        "    print(\"üìê Feature matrix:\", X.shape, \"from\", os.path.basename(tflite_path))\n",
        "    return X, y\n",
        "\n",
        "print(\"‚úÖ Using FULL TFLite for feature extraction:\", EFFICIENTNET_TFLITE_FP16)\n",
        "Xe_tr, ye_tr = extract_features_from_full_tflite(EFFICIENTNET_TFLITE_FP16, train_df)\n",
        "Xe_va, ye_va = extract_features_from_full_tflite(EFFICIENTNET_TFLITE_FP16, valid_df)\n",
        "Xe_te, ye_te = extract_features_from_full_tflite(EFFICIENTNET_TFLITE_FP16, test_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmSnYRejnhDd",
        "outputId": "45123ae8-392c-4f52-f5cb-827b99ab8c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using FULL TFLite for feature extraction: /content/drive/MyDrive/DiabeticProject/tflite/EfficientNetV2B0_model_fp16.tflite\n",
            "üîé Using feature tensor idx=739 name=functional_1_1/efficientnetv2-b0_1/max_pool_1/Max shape=[   1 1280]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìê Feature matrix: (1242, 1280) from EfficientNetV2B0_model_fp16.tflite\n",
            "üîé Using feature tensor idx=739 name=functional_1_1/efficientnetv2-b0_1/max_pool_1/Max shape=[   1 1280]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìê Feature matrix: (265, 1280) from EfficientNetV2B0_model_fp16.tflite\n",
            "üîé Using feature tensor idx=739 name=functional_1_1/efficientnetv2-b0_1/max_pool_1/Max shape=[   1 1280]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìê Feature matrix: (265, 1280) from EfficientNetV2B0_model_fp16.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 5B (dimension-safe): recover/train heads ‚Üí auto-fix dim mismatch ‚Üí distill/export head TFLite ‚Üí time =====\n",
        "import os, glob, time, json, joblib, numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.base import clone\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# ---------- 0) Resolve backbone + features (prefer EfficientNet) ----------\n",
        "if 'EFFICIENTNET_TFLITE_FP16' in globals() and EFFICIENTNET_TFLITE_FP16 and os.path.exists(EFFICIENTNET_TFLITE_FP16):\n",
        "    BACKBONE_TFLITE = EFFICIENTNET_TFLITE_FP16\n",
        "    TAG = \"EfficientNetV2B0_fp16_FE\"\n",
        "    X_tr = globals().get('Xe_tr'); X_va = globals().get('Xe_va')\n",
        "    y_tr = globals().get('ye_tr'); y_va = globals().get('ye_va')\n",
        "    expected_feat_dim = 1280  # EfficientNetV2B0 pooled width\n",
        "else:\n",
        "    assert 'DENSENET_TFLITE_FP16' in globals() and os.path.exists(DENSENET_TFLITE_FP16), \\\n",
        "        \"No EfficientNet or DenseNet backbone TFLite found.\"\n",
        "    BACKBONE_TFLITE = DENSENET_TFLITE_FP16\n",
        "    TAG = \"DenseNet121_fp16_FE\"\n",
        "    X_tr = globals().get('Xd_tr'); X_va = globals().get('Xd_va')\n",
        "    y_tr = globals().get('yd_tr'); y_va = globals().get('yd_va')\n",
        "    expected_feat_dim = 1024  # DenseNet121 pooled width\n",
        "\n",
        "assert isinstance(X_tr, np.ndarray) and isinstance(X_va, np.ndarray), \"Feature matrices (X_tr/X_va) not found.\"\n",
        "assert isinstance(y_tr, np.ndarray) and isinstance(y_va, np.ndarray), \"Label vectors (y_tr/y_va) not found.\"\n",
        "assert 'test_df' in globals() and len(test_df) > 0 and 'image_path' in test_df.columns, \"test_df with image_path required.\"\n",
        "\n",
        "SAVE_DIR = HYBRID_SAVE_DIR if 'HYBRID_SAVE_DIR' in globals() else \"./hybrid_models\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- 1) Recover/Train heads ----------\n",
        "heads = {}\n",
        "# (a) in-RAM\n",
        "if 'heads' in globals() and isinstance(heads, dict) and heads:\n",
        "    pass\n",
        "else:\n",
        "    if 'knn' in globals(): heads['KNN'] = knn\n",
        "    if 'svm' in globals(): heads['SVM_RBF'] = svm\n",
        "    if 'rf'  in globals(): heads['RandomForest'] = rf\n",
        "\n",
        "# (b) from disk\n",
        "def _try_load_head(path):\n",
        "    try:\n",
        "        return joblib.load(path)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "if not heads:\n",
        "    cand_paths = [\n",
        "        os.path.join(SAVE_DIR, f\"{TAG}_KNN.joblib\"),\n",
        "        os.path.join(SAVE_DIR, f\"{TAG}_SVM_RBF.joblib\"),\n",
        "        os.path.join(SAVE_DIR, f\"{TAG}_RandomForest.joblib\"),\n",
        "    ]\n",
        "    cand_paths += glob.glob(os.path.join(SAVE_DIR, \"*_KNN.joblib\"))\n",
        "    cand_paths += glob.glob(os.path.join(SAVE_DIR, \"*_SVM_RBF.joblib\"))\n",
        "    cand_paths += glob.glob(os.path.join(SAVE_DIR, \"*_RandomForest.joblib\"))\n",
        "    for p in cand_paths:\n",
        "        est = _try_load_head(p)\n",
        "        if est is None: continue\n",
        "        if p.endswith(\"KNN.joblib\"): heads.setdefault(\"KNN\", est)\n",
        "        elif p.endswith(\"SVM_RBF.joblib\"): heads.setdefault(\"SVM_RBF\", est)\n",
        "        elif p.endswith(\"RandomForest.joblib\"): heads.setdefault(\"RandomForest\", est)\n",
        "\n",
        "# (c) train if still missing\n",
        "d_in = X_tr.shape[1]\n",
        "pca_n_default = 128 if d_in > 128 else None\n",
        "\n",
        "def _mk_steps(use_pca=True, pca_n=pca_n_default):\n",
        "    steps = [(\"scaler\", StandardScaler())]\n",
        "    if use_pca and pca_n:\n",
        "        steps.append((\"pca\", PCA(n_components=pca_n, random_state=42)))\n",
        "    return steps\n",
        "\n",
        "X_train = np.vstack([X_tr, X_va])\n",
        "y_train = np.concatenate([y_tr, y_va])\n",
        "\n",
        "if not heads:\n",
        "    print(\"‚ÑπÔ∏è No heads found in RAM/disk ‚Äî training KNN / SVM_RBF / RF on current features ‚Ä¶\")\n",
        "    knn = Pipeline(_mk_steps(True) + [(\"knn\", KNeighborsClassifier(n_neighbors=5, weights=\"distance\", n_jobs=-1))]).fit(X_train, y_train)\n",
        "    svm = Pipeline(_mk_steps(True) + [(\"svm\", SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True, random_state=42))]).fit(X_train, y_train)\n",
        "    rf  = Pipeline(_mk_steps(True) + [(\"rf\", RandomForestClassifier(\n",
        "            n_estimators=600, max_depth=None, min_samples_leaf=1,\n",
        "            class_weight=\"balanced_subsample\", n_jobs=-1, random_state=42\n",
        "         ))]).fit(X_train, y_train)\n",
        "    heads = {\"KNN\": knn, \"SVM_RBF\": svm, \"RandomForest\": rf}\n",
        "    joblib.dump(knn, os.path.join(SAVE_DIR, f\"{TAG}_KNN.joblib\"))\n",
        "    joblib.dump(svm, os.path.join(SAVE_DIR, f\"{TAG}_SVM_RBF.joblib\"))\n",
        "    joblib.dump(rf,  os.path.join(SAVE_DIR, f\"{TAG}_RandomForest.joblib\"))\n",
        "    print(\"‚úÖ Heads trained & saved to:\", SAVE_DIR)\n",
        "\n",
        "# ---------- 2) Auto-fix any feature-dimension mismatches (refit on current features) ----------\n",
        "def _needs_refit(pipe, x_width):\n",
        "    # check scaler mean/scale width or PCA components width\n",
        "    scaler = pipe.named_steps.get(\"scaler\")\n",
        "    pca    = pipe.named_steps.get(\"pca\")\n",
        "    if scaler is not None:\n",
        "        mean = getattr(scaler, \"mean_\", None)\n",
        "        if mean is not None and mean.shape[0] != x_width:\n",
        "            return True\n",
        "    if pca is not None:\n",
        "        comps = getattr(pca, \"components_\", None)\n",
        "        if comps is not None and comps.shape[1] != x_width:\n",
        "            return True\n",
        "    # also check n_features_in_ if present\n",
        "    nfi = getattr(pipe, \"n_features_in_\", None)\n",
        "    if nfi is not None and nfi != x_width:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _refit_like(pipe, X, y):\n",
        "    # reconstruct a similar pipeline (scaler + (optional PCA) + last estimator with same params)\n",
        "    steps = []\n",
        "    if \"scaler\" in pipe.named_steps:\n",
        "        steps.append((\"scaler\", StandardScaler()))\n",
        "    if \"pca\" in pipe.named_steps:\n",
        "        # keep same n_components if valid else clamp\n",
        "        old_pca = pipe.named_steps[\"pca\"]\n",
        "        n_comp = getattr(old_pca, \"n_components\", pca_n_default)\n",
        "        if isinstance(n_comp, int):\n",
        "            n_comp = min(n_comp, X.shape[1])\n",
        "        steps.append((\"pca\", PCA(n_components=n_comp, random_state=42)))\n",
        "    # clone last estimator\n",
        "    last_name, last_est = list(pipe.named_steps.items())[-1]\n",
        "    est = clone(last_est)\n",
        "    new_pipe = Pipeline(steps + [(last_name, est)])\n",
        "    return new_pipe.fit(X, y)\n",
        "\n",
        "for name in list(heads.keys()):\n",
        "    if _needs_refit(heads[name], X_train.shape[1]):\n",
        "        print(f\"‚ôªÔ∏è Re-fitting {name} to match current feature width {X_train.shape[1]} ‚Ä¶\")\n",
        "        heads[name] = _refit_like(heads[name], X_train, y_train)\n",
        "        # persist\n",
        "        joblib.dump(heads[name], os.path.join(SAVE_DIR, f\"{TAG}_{name}.joblib\"))\n",
        "\n",
        "# ---------- 3) Pick final head (best f1_macro if available, else SVM‚ÜíKNN‚ÜíRF) ----------\n",
        "def _pick_best_head_from_metrics(metrics_df, tag):\n",
        "    mdf = metrics_df.copy()\n",
        "    if not {\"model\",\"f1_macro\"} <= set(mdf.columns): return None\n",
        "    mdf = mdf[mdf[\"model\"].astype(str).str.startswith(tag + \"+\")]\n",
        "    if mdf.empty: return None\n",
        "    mdf = mdf.assign(head_name=mdf[\"model\"].str.split(\"+\").str[-1])\n",
        "    return mdf.sort_values(\"f1_macro\", ascending=False)[\"head_name\"].iloc[0]\n",
        "\n",
        "final_pipe = None\n",
        "if 'metrics_df' in globals():\n",
        "    best = _pick_best_head_from_metrics(metrics_df, TAG)\n",
        "    if best in heads: final_pipe = heads[best]\n",
        "if final_pipe is None:\n",
        "    for cand in (\"SVM_RBF\",\"KNN\",\"RandomForest\"):\n",
        "        if cand in heads: final_pipe = heads[cand]; break\n",
        "assert final_pipe is not None, \"Could not select a final head.\"\n",
        "\n",
        "# ---------- 4) Distill chosen head ‚Üí LR on its transformed space ----------\n",
        "def _transform(pipe, X):\n",
        "    Z = X\n",
        "    if \"scaler\" in pipe.named_steps: Z = pipe.named_steps[\"scaler\"].transform(Z)\n",
        "    if \"pca\"    in pipe.named_steps: Z = pipe.named_steps[\"pca\"].transform(Z)\n",
        "    return Z\n",
        "\n",
        "try:\n",
        "    prob = final_pipe.predict_proba(X_train)[:, 1]\n",
        "    y_hard = (np.clip(prob, 1e-6, 1-1e-6) >= 0.5).astype(int)\n",
        "    Z_train = _transform(final_pipe, X_train)\n",
        "    lr = LogisticRegression(max_iter=2000, random_state=42).fit(Z_train, y_hard)\n",
        "except Exception:\n",
        "    Z_train = _transform(final_pipe, X_train)\n",
        "    lr = LogisticRegression(max_iter=2000, random_state=42).fit(Z_train, y_train)\n",
        "\n",
        "# Fuse scaler + PCA + LR back to raw feature basis\n",
        "scaler = final_pipe.named_steps.get(\"scaler\")\n",
        "pca    = final_pipe.named_steps.get(\"pca\")\n",
        "scale = getattr(scaler, \"scale_\", None) if scaler is not None else None\n",
        "mean  = getattr(scaler, \"mean_\",  None) if scaler is not None else None\n",
        "P     = pca.components_ if pca is not None else None\n",
        "\n",
        "Wlr = lr.coef_.reshape(-1,1).astype(np.float32)\n",
        "blr = float(lr.intercept_.ravel()[0])\n",
        "Wp  = (P.T @ Wlr).astype(np.float32) if P is not None else Wlr\n",
        "Wf  = (Wp * (1.0 / scale.reshape(-1,1))).astype(np.float32) if scale is not None else Wp\n",
        "b_shift = - float((Wf.ravel() * mean).sum()) if mean is not None else 0.0\n",
        "bf = np.array([blr + b_shift], dtype=np.float32)\n",
        "\n",
        "D_in = Wf.shape[0]\n",
        "print(f\"Head fused: input dim = {D_in}, weights shape = {Wf.shape}, bias = {bf.shape}\")\n",
        "\n",
        "# ---------- 5) Export distilled head as standalone TFLite ----------\n",
        "head_tfl_path = os.path.join(SAVE_DIR, f\"{TAG}_HEAD_fused_fp32.tflite\")\n",
        "\n",
        "class HeadDenseSigmoid(tf.Module):\n",
        "    def __init__(self, W, b):\n",
        "        super().__init__()\n",
        "        self.W = tf.Variable(W, trainable=False, dtype=tf.float32, name=\"W\")\n",
        "        self.b = tf.Variable(b, trainable=False, dtype=tf.float32, name=\"b\")\n",
        "    @tf.function(input_signature=[tf.TensorSpec([1, D_in], tf.float32, name=\"features\")])\n",
        "    def __call__(self, features):\n",
        "        logits = tf.linalg.matmul(features, self.W) + self.b\n",
        "        prob = tf.math.sigmoid(logits)\n",
        "        return {\"prob\": prob}\n",
        "\n",
        "_module = HeadDenseSigmoid(Wf, bf)\n",
        "_concrete = _module.__call__.get_concrete_function()\n",
        "_converter = tf.lite.TFLiteConverter.from_concrete_functions([_concrete], trackable_obj=_module)\n",
        "head_tfl_bytes = _converter.convert()\n",
        "with open(head_tfl_path, \"wb\") as f:\n",
        "    f.write(head_tfl_bytes)\n",
        "print(\"‚úÖ Wrote HEAD TFLite:\", head_tfl_path, \"|\", round(os.path.getsize(head_tfl_path)/(1024*1024), 3), \"MB\")\n",
        "\n",
        "# ---------- 6) Timing helpers ----------\n",
        "def _load_interp(path):\n",
        "    inter = tf.lite.Interpreter(model_path=path, num_threads=os.cpu_count() or 1)\n",
        "    inter.allocate_tensors()\n",
        "    return inter\n",
        "\n",
        "def _prep_img(img_path, W, H, dtype):\n",
        "    img = Image.open(img_path).convert(\"RGB\").resize((W, H), Image.BILINEAR)\n",
        "    x = np.asarray(img, dtype=np.float32)/255.0\n",
        "    x = np.expand_dims(x, 0)\n",
        "    if dtype == np.float16: x = x.astype(np.float16)\n",
        "    elif dtype == np.float32: x = x.astype(np.float32)\n",
        "    else: x = x.astype(dtype)\n",
        "    return x\n",
        "\n",
        "def _find_feature_index(inter, expected_dim):\n",
        "    details = inter.get_tensor_details()\n",
        "    sel = None\n",
        "    # name-first heuristic\n",
        "    for d in details:\n",
        "        name = d[\"name\"].decode() if isinstance(d[\"name\"], bytes) else d[\"name\"]\n",
        "        shp  = tuple(int(s) for s in (d.get(\"shape_signature\", d[\"shape\"])))\n",
        "        if len(shp) in (1,2) and any(k in name.lower() for k in (\"global_average_pool\",\"global_max_pool\",\"avg_pool\",\"mean\",\"gap\",\"pool\")):\n",
        "            sel = d\n",
        "    # dim fallback\n",
        "    if sel is None:\n",
        "        for d in details:\n",
        "            shp  = tuple(int(s) for s in (d.get(\"shape_signature\", d[\"shape\"])))\n",
        "            if len(shp) in (1,2) and shp[-1] == expected_dim:\n",
        "                sel = d; break\n",
        "    if sel is None:\n",
        "        sel = inter.get_output_details()[0]\n",
        "    return sel[\"index\"]\n",
        "\n",
        "# ---------- 7) Build 10-image sample & time end-to-end ----------\n",
        "sample_paths = list(test_df[\"image_path\"])[:10] if len(test_df) >= 10 else list(test_df[\"image_path\"])\n",
        "assert len(sample_paths) > 0, \"No images found in test_df['image_path'] to time.\"\n",
        "\n",
        "backbone = _load_interp(BACKBONE_TFLITE)\n",
        "b_in = backbone.get_input_details()[0]\n",
        "H, W = int(b_in[\"shape\"][1]), int(b_in[\"shape\"][2])\n",
        "feat_idx = _find_feature_index(backbone, expected_feat_dim)\n",
        "\n",
        "head = _load_interp(head_tfl_path)\n",
        "h_in = head.get_input_details()[0]\n",
        "\n",
        "# warm-up\n",
        "for _ in range(3):\n",
        "    a = _prep_img(sample_paths[0], W, H, b_in[\"dtype\"])\n",
        "    backbone.set_tensor(b_in[\"index\"], a); backbone.invoke()\n",
        "    f = backbone.get_tensor(feat_idx).astype(np.float32).reshape(1, D_in)\n",
        "    head.set_tensor(h_in[\"index\"], f); head.invoke()\n",
        "\n",
        "times = []\n",
        "for p in sample_paths:\n",
        "    a = _prep_img(p, W, H, b_in[\"dtype\"])\n",
        "    t0 = time.perf_counter()\n",
        "    backbone.set_tensor(b_in[\"index\"], a); backbone.invoke()\n",
        "    f = backbone.get_tensor(feat_idx).astype(np.float32).reshape(1, D_in)\n",
        "    head.set_tensor(h_in[\"index\"], f); head.invoke()\n",
        "    _ = head.get_output_details()[0]\n",
        "    t1 = time.perf_counter()\n",
        "    times.append((t1 - t0) * 1000.0)\n",
        "\n",
        "t = np.array(times, np.float64)\n",
        "lat = {\"N\": len(sample_paths), \"avg_ms\": round(t.mean(), 3),\n",
        "       \"std_ms\": round(t.std(ddof=1), 3) if len(t)>1 else 0.0,\n",
        "       \"min_ms\": round(t.min(), 3), \"max_ms\": round(t.max(), 3)}\n",
        "df_latency = pd.DataFrame([{**{\"BackboneTFLite\": BACKBONE_TFLITE, \"HeadTFLite\": head_tfl_path, \"TAG\": TAG}, **lat}])\n",
        "display(df_latency)\n",
        "\n",
        "lat_csv = os.path.join(SAVE_DIR, f\"{TAG}_ALL_TFLite_hybrid_latency.csv\")\n",
        "df_latency.to_csv(lat_csv, index=False)\n",
        "with open(os.path.join(SAVE_DIR, f\"{TAG}_ALL_TFLite_hybrid_manifest.json\"), \"w\") as f:\n",
        "    json.dump({\"backbone_tflite\": BACKBONE_TFLITE,\n",
        "               \"head_tflite\": head_tfl_path,\n",
        "               \"feature_dim\": int(D_in),\n",
        "               \"images_timed\": len(sample_paths),\n",
        "               \"latency_ms\": lat}, f, indent=2)\n",
        "\n",
        "print(\"\\n‚úÖ Saved:\")\n",
        "print(\" -\", head_tfl_path)\n",
        "print(\" -\", lat_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "DaHEE8RtqgWC",
        "outputId": "55d8559d-090d-49e7-a19c-76d45d3f8f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ôªÔ∏è Re-fitting KNN to match current feature width 1280 ‚Ä¶\n",
            "‚ôªÔ∏è Re-fitting SVM_RBF to match current feature width 1280 ‚Ä¶\n",
            "‚ôªÔ∏è Re-fitting RandomForest to match current feature width 1280 ‚Ä¶\n",
            "Head fused: input dim = 1280, weights shape = (1280, 1), bias = (1,)\n",
            "‚úÖ Wrote HEAD TFLite: /content/drive/MyDrive/DiabeticProject/hybrid_models/EfficientNetV2B0_fp16_FE_HEAD_fused_fp32.tflite | 0.006 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                      BackboneTFLite  \\\n",
              "0  /content/drive/MyDrive/DiabeticProject/tflite/...   \n",
              "\n",
              "                                          HeadTFLite  \\\n",
              "0  /content/drive/MyDrive/DiabeticProject/hybrid_...   \n",
              "\n",
              "                        TAG   N  avg_ms  std_ms  min_ms  max_ms  \n",
              "0  EfficientNetV2B0_fp16_FE  10   32.11   1.464  30.566  35.815  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ae892b2-24e8-4c69-8857-d87b881737d7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BackboneTFLite</th>\n",
              "      <th>HeadTFLite</th>\n",
              "      <th>TAG</th>\n",
              "      <th>N</th>\n",
              "      <th>avg_ms</th>\n",
              "      <th>std_ms</th>\n",
              "      <th>min_ms</th>\n",
              "      <th>max_ms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/DiabeticProject/tflite/...</td>\n",
              "      <td>/content/drive/MyDrive/DiabeticProject/hybrid_...</td>\n",
              "      <td>EfficientNetV2B0_fp16_FE</td>\n",
              "      <td>10</td>\n",
              "      <td>32.11</td>\n",
              "      <td>1.464</td>\n",
              "      <td>30.566</td>\n",
              "      <td>35.815</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ae892b2-24e8-4c69-8857-d87b881737d7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9ae892b2-24e8-4c69-8857-d87b881737d7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9ae892b2-24e8-4c69-8857-d87b881737d7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_edb9196e-15d8-43c7-bd25-f674e6a0acad\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_latency')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_edb9196e-15d8-43c7-bd25-f674e6a0acad button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_latency');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_latency",
              "summary": "{\n  \"name\": \"df_latency\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"BackboneTFLite\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"/content/drive/MyDrive/DiabeticProject/tflite/EfficientNetV2B0_model_fp16.tflite\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HeadTFLite\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"/content/drive/MyDrive/DiabeticProject/hybrid_models/EfficientNetV2B0_fp16_FE_HEAD_fused_fp32.tflite\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TAG\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"EfficientNetV2B0_fp16_FE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 10,\n        \"max\": 10,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 32.11,\n        \"max\": 32.11,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32.11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.464,\n        \"max\": 1.464,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.464\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 30.566,\n        \"max\": 30.566,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          30.566\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 35.815,\n        \"max\": 35.815,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          35.815\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Saved:\n",
            " - /content/drive/MyDrive/DiabeticProject/hybrid_models/EfficientNetV2B0_fp16_FE_HEAD_fused_fp32.tflite\n",
            " - /content/drive/MyDrive/DiabeticProject/hybrid_models/EfficientNetV2B0_fp16_FE_ALL_TFLite_hybrid_latency.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 5C (EfficientNetV2B0): Export THREE head TFLites (from KNN/SVM/RF) + optional timing =====\n",
        "import os, glob, time, json, joblib, numpy as np, pandas as pd, tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.base import clone\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# ---------- Config / paths ----------\n",
        "assert 'EFFICIENTNET_TFLITE_FP16' in globals() and os.path.exists(EFFICIENTNET_TFLITE_FP16), \\\n",
        "    \"EFFICIENTNET_TFLITE_FP16 not found. Make sure Cell 4 for EfficientNet ran.\"\n",
        "BACKBONE_TFLITE = EFFICIENTNET_TFLITE_FP16\n",
        "TAG = \"EfficientNetV2B0_fp16_FE\"\n",
        "SAVE_DIR = HYBRID_SAVE_DIR if 'HYBRID_SAVE_DIR' in globals() else \"./hybrid_models\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Expect features from Cell 4:\n",
        "X_tr = globals().get('Xe_tr'); X_va = globals().get('Xe_va')\n",
        "y_tr = globals().get('ye_tr'); y_va = globals().get('ye_va')\n",
        "assert isinstance(X_tr, np.ndarray) and isinstance(X_va, np.ndarray), \"Xe_tr/Xe_va missing.\"\n",
        "assert isinstance(y_tr, np.ndarray) and isinstance(y_va, np.ndarray), \"ye_tr/ye_va missing.\"\n",
        "assert 'test_df' in globals() and len(test_df) > 0 and 'image_path' in test_df.columns, \"test_df with image_path required.\"\n",
        "\n",
        "X_train = np.vstack([X_tr, X_va])\n",
        "y_train = np.concatenate([y_tr, y_va])\n",
        "d_in = X_train.shape[1]            # should be 1280 for EfficientNetV2B0 pooled features\n",
        "pca_n_default = 128 if d_in > 128 else None\n",
        "\n",
        "# ---------- Recover heads (KNN/SVM_RBF/RF) from RAM or disk ----------\n",
        "heads = {}\n",
        "if 'heads' in globals() and isinstance(heads, dict) and heads:\n",
        "    pass\n",
        "else:\n",
        "    if 'knn' in globals(): heads['KNN'] = knn\n",
        "    if 'svm' in globals(): heads['SVM_RBF'] = svm\n",
        "    if 'rf'  in globals(): heads['RandomForest'] = rf\n",
        "\n",
        "def _try_load_head(path):\n",
        "    try:\n",
        "        return joblib.load(path)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "if not heads:\n",
        "    cand = []\n",
        "    cand += [os.path.join(SAVE_DIR, f\"{TAG}_KNN.joblib\"),\n",
        "             os.path.join(SAVE_DIR, f\"{TAG}_SVM_RBF.joblib\"),\n",
        "             os.path.join(SAVE_DIR, f\"{TAG}_RandomForest.joblib\")]\n",
        "    cand += glob.glob(os.path.join(SAVE_DIR, \"*_KNN.joblib\"))\n",
        "    cand += glob.glob(os.path.join(SAVE_DIR, \"*_SVM_RBF.joblib\"))\n",
        "    cand += glob.glob(os.path.join(SAVE_DIR, \"*_RandomForest.joblib\"))\n",
        "    for p in cand:\n",
        "        est = _try_load_head(p)\n",
        "        if est is None: continue\n",
        "        if p.endswith(\"KNN.joblib\"): heads.setdefault(\"KNN\", est)\n",
        "        elif p.endswith(\"SVM_RBF.joblib\"): heads.setdefault(\"SVM_RBF\", est)\n",
        "        elif p.endswith(\"RandomForest.joblib\"): heads.setdefault(\"RandomForest\", est)\n",
        "\n",
        "# If still missing, train simple versions now (uses same steps as earlier cell)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def _mk_steps(use_pca=True, pca_n=pca_n_default):\n",
        "    steps = [(\"scaler\", StandardScaler())]\n",
        "    if use_pca and pca_n:\n",
        "        steps.append((\"pca\", PCA(n_components=pca_n, random_state=42)))\n",
        "    return steps\n",
        "\n",
        "if not heads:\n",
        "    print(\"‚ÑπÔ∏è Training heads on EfficientNet features ‚Ä¶\")\n",
        "    knn = Pipeline(_mk_steps(True) + [(\"knn\", KNeighborsClassifier(n_neighbors=5, weights=\"distance\", n_jobs=-1))]).fit(X_train, y_train)\n",
        "    svm = Pipeline(_mk_steps(True) + [(\"svm\", SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True, random_state=42))]).fit(X_train, y_train)\n",
        "    rf  = Pipeline(_mk_steps(True) + [(\"rf\", RandomForestClassifier(\n",
        "            n_estimators=600, max_depth=None, min_samples_leaf=1,\n",
        "            class_weight=\"balanced_subsample\", n_jobs=-1, random_state=42\n",
        "         ))]).fit(X_train, y_train)\n",
        "    heads = {\"KNN\": knn, \"SVM_RBF\": svm, \"RandomForest\": rf}\n",
        "    joblib.dump(knn, os.path.join(SAVE_DIR, f\"{TAG}_KNN.joblib\"))\n",
        "    joblib.dump(svm, os.path.join(SAVE_DIR, f\"{TAG}_SVM_RBF.joblib\"))\n",
        "    joblib.dump(rf,  os.path.join(SAVE_DIR, f\"{TAG}_RandomForest.joblib\"))\n",
        "    print(\"‚úÖ Heads trained & saved.\")\n",
        "\n",
        "# ---------- Ensure each head matches current feature width; refit if needed ----------\n",
        "def _needs_refit(pipe, xw):\n",
        "    scaler = pipe.named_steps.get(\"scaler\")\n",
        "    pca    = pipe.named_steps.get(\"pca\")\n",
        "    if scaler is not None:\n",
        "        mean = getattr(scaler, \"mean_\", None)\n",
        "        if mean is not None and mean.shape[0] != xw:\n",
        "            return True\n",
        "    if pca is not None:\n",
        "        comps = getattr(pca, \"components_\", None)\n",
        "        if comps is not None and comps.shape[1] != xw:\n",
        "            return True\n",
        "    nfi = getattr(pipe, \"n_features_in_\", None)\n",
        "    return (nfi is not None and nfi != xw)\n",
        "\n",
        "def _refit_like(pipe, X, y):\n",
        "    steps = []\n",
        "    if \"scaler\" in pipe.named_steps:\n",
        "        steps.append((\"scaler\", StandardScaler()))\n",
        "    if \"pca\" in pipe.named_steps:\n",
        "        old_pca = pipe.named_steps[\"pca\"]\n",
        "        n_comp = getattr(old_pca, \"n_components\", pca_n_default)\n",
        "        if isinstance(n_comp, int): n_comp = min(n_comp, X.shape[1])\n",
        "        steps.append((\"pca\", PCA(n_components=n_comp, random_state=42)))\n",
        "    last_name, last_est = list(pipe.named_steps.items())[-1]\n",
        "    new_pipe = Pipeline(steps + [(last_name, clone(last_est))]).fit(X, y)\n",
        "    return new_pipe\n",
        "\n",
        "for name in list(heads.keys()):\n",
        "    if _needs_refit(heads[name], X_train.shape[1]):\n",
        "        print(f\"‚ôªÔ∏è Re-fitting {name} on EfficientNet features (width {X_train.shape[1]}) ‚Ä¶\")\n",
        "        heads[name] = _refit_like(heads[name], X_train, y_train)\n",
        "        joblib.dump(heads[name], os.path.join(SAVE_DIR, f\"{TAG}_{name}.joblib\"))\n",
        "\n",
        "# ---------- Distillation utils (to LR) & head TFLite export ----------\n",
        "def _transform(pipe, X):\n",
        "    Z = X\n",
        "    if \"scaler\" in pipe.named_steps: Z = pipe.named_steps[\"scaler\"].transform(Z)\n",
        "    if \"pca\"    in pipe.named_steps: Z = pipe.named_steps[\"pca\"].transform(Z)\n",
        "    return Z\n",
        "\n",
        "def fuse_head(pipe, X_train, y_train):\n",
        "    Z_train = _transform(pipe, X_train)\n",
        "    lr = LogisticRegression(max_iter=2000, random_state=42)\n",
        "    try:\n",
        "        prob = pipe.predict_proba(X_train)[:, 1]\n",
        "        y_hard = (np.clip(prob, 1e-6, 1-1e-6) >= 0.5).astype(int)\n",
        "        lr.fit(Z_train, y_hard)\n",
        "    except Exception:\n",
        "        lr.fit(Z_train, y_train)\n",
        "\n",
        "    scaler = pipe.named_steps.get(\"scaler\")\n",
        "    pca    = pipe.named_steps.get(\"pca\")\n",
        "    scale = getattr(scaler, \"scale_\", None) if scaler is not None else None\n",
        "    mean  = getattr(scaler, \"mean_\",  None) if scaler is not None else None\n",
        "    P     = pca.components_ if pca is not None else None\n",
        "\n",
        "    Wlr = lr.coef_.reshape(-1,1).astype(np.float32)\n",
        "    blr = float(lr.intercept_.ravel()[0])\n",
        "\n",
        "    Wp  = (P.T @ Wlr).astype(np.float32) if P is not None else Wlr\n",
        "    Wf  = (Wp * (1.0 / scale.reshape(-1,1))).astype(np.float32) if scale is not None else Wp\n",
        "    b_shift = - float((Wf.ravel() * mean).sum()) if mean is not None else 0.0\n",
        "    bf = np.array([blr + b_shift], dtype=np.float32)\n",
        "    return Wf, bf\n",
        "\n",
        "def export_head_tflite(Wf, bf, out_path):\n",
        "    D_in = Wf.shape[0]\n",
        "    class HeadDenseSigmoid(tf.Module):\n",
        "        def __init__(self, W, b):\n",
        "            super().__init__()\n",
        "            self.W = tf.Variable(W, trainable=False, dtype=tf.float32, name=\"W\")\n",
        "            self.b = tf.Variable(b, trainable=False, dtype=tf.float32, name=\"b\")\n",
        "        @tf.function(input_signature=[tf.TensorSpec([1, D_in], tf.float32, name=\"features\")])\n",
        "        def __call__(self, features):\n",
        "            logits = tf.linalg.matmul(features, self.W) + self.b\n",
        "            prob = tf.math.sigmoid(logits)\n",
        "            return {\"prob\": prob}\n",
        "    module = HeadDenseSigmoid(Wf, bf)\n",
        "    concrete = module.__call__.get_concrete_function()\n",
        "    conv = tf.lite.TFLiteConverter.from_concrete_functions([concrete], trackable_obj=module)\n",
        "    tbytes = conv.convert()\n",
        "    with open(out_path, \"wb\") as f: f.write(tbytes)\n",
        "    print(\"üíæ Head TFLite:\", out_path, \"|\", round(os.path.getsize(out_path)/(1024*1024),3), \"MB\")\n",
        "    return out_path\n",
        "\n",
        "# ---------- Export all three heads ----------\n",
        "head_paths = {}\n",
        "for name, pipe in heads.items():\n",
        "    Wf, bf = fuse_head(pipe, X_train, y_train)\n",
        "    out_path = os.path.join(SAVE_DIR, f\"{TAG}_HEAD_from_{name}_fused_fp32.tflite\")\n",
        "    head_paths[name] = export_head_tflite(Wf, bf, out_path)\n",
        "\n",
        "# ---------- Optional: timing backbone + each head TFLite on 10 images ----------\n",
        "def _prep_img(p, W, H, dtype):\n",
        "    img = Image.open(p).convert(\"RGB\").resize((W, H), Image.BILINEAR)\n",
        "    x = np.asarray(img, dtype=np.float32)/255.0\n",
        "    x = np.expand_dims(x, 0)\n",
        "    if dtype == np.float16: x = x.astype(np.float16)\n",
        "    elif dtype == np.float32: x = x.astype(np.float32)\n",
        "    else: x = x.astype(dtype)\n",
        "    return x\n",
        "\n",
        "def _find_feat_idx(inter, d_in_guess):\n",
        "    details = inter.get_tensor_details()\n",
        "    sel = None\n",
        "    for d in details:\n",
        "        name = d[\"name\"].decode() if isinstance(d[\"name\"], bytes) else d[\"name\"]\n",
        "        shp = tuple(int(s) for s in (d.get(\"shape_signature\", d[\"shape\"])))\n",
        "        if len(shp) in (1,2) and any(k in name.lower() for k in (\"global_average_pool\",\"global_max_pool\",\n",
        "                                                                 \"avg_pool\",\"mean\",\"gap\",\"pool\")):\n",
        "            sel = d\n",
        "    if sel is None:\n",
        "        for d in details:\n",
        "            shp = tuple(int(s) for s in (d.get(\"shape_signature\", d[\"shape\"])))\n",
        "            if len(shp) in (1,2) and shp[-1] == d_in_guess:\n",
        "                sel = d; break\n",
        "    return sel[\"index\"] if sel is not None else inter.get_output_details()[0][\"index\"]\n",
        "\n",
        "def time_two_tflites(backbone_path, head_path, img_paths, warmups=3, d_in_guess=d_in):\n",
        "    b = tf.lite.Interpreter(model_path=backbone_path); b.allocate_tensors()\n",
        "    bi = b.get_input_details()[0]; H,W = int(bi[\"shape\"][1]), int(bi[\"shape\"][2])\n",
        "    feat_idx = _find_feat_idx(b, d_in_guess=d_in_guess)\n",
        "    h = tf.lite.Interpreter(model_path=head_path); h.allocate_tensors()\n",
        "    hi = h.get_input_details()[0]\n",
        "\n",
        "    for _ in range(warmups):\n",
        "        a = _prep_img(img_paths[0], W, H, bi[\"dtype\"])\n",
        "        b.set_tensor(bi[\"index\"], a); b.invoke()\n",
        "        f = b.get_tensor(feat_idx).astype(np.float32).reshape(1, d_in_guess)\n",
        "        h.set_tensor(hi[\"index\"], f); h.invoke()\n",
        "\n",
        "    times=[]\n",
        "    for p in img_paths:\n",
        "        a = _prep_img(p, W, H, bi[\"dtype\"])\n",
        "        t0 = time.perf_counter()\n",
        "        b.set_tensor(bi[\"index\"], a); b.invoke()\n",
        "        f = b.get_tensor(feat_idx).astype(np.float32).reshape(1, d_in_guess)\n",
        "        h.set_tensor(hi[\"index\"], f); h.invoke()\n",
        "        _ = h.get_output_details()[0]\n",
        "        t1 = time.perf_counter()\n",
        "        times.append((t1-t0)*1000.0)\n",
        "\n",
        "    t = np.array(times, np.float64)\n",
        "    return dict(N=len(img_paths), avg_ms=round(t.mean(),3),\n",
        "                std_ms=round(t.std(ddof=1),3) if len(t)>1 else 0.0,\n",
        "                min_ms=round(t.min(),3), max_ms=round(t.max(),3))\n",
        "\n",
        "sample_paths = list(test_df[\"image_path\"])[:10] if len(test_df) >= 10 else list(test_df[\"image_path\"])\n",
        "timing_rows = []\n",
        "for name, hpath in head_paths.items():\n",
        "    metrics = time_two_tflites(BACKBONE_TFLITE, hpath, sample_paths, warmups=3, d_in_guess=d_in)\n",
        "    timing_rows.append({\"Head\": name, \"BackboneTFLite\": BACKBONE_TFLITE, \"HeadTFLite\": hpath, **metrics})\n",
        "\n",
        "df_timing = pd.DataFrame(timing_rows)\n",
        "display(df_timing)\n",
        "csv_out = os.path.join(SAVE_DIR, f\"{TAG}_ALL_heads_latency.csv\")\n",
        "df_timing.to_csv(csv_out, index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Saved head files & timing CSV in:\", SAVE_DIR)\n",
        "for name, p in head_paths.items():\n",
        "    print(f\" - {name}: {p}\")\n",
        "print(\" - timing:\", csv_out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "K9d3J1FhtFpo",
        "outputId": "abdb75d4-2e77-4961-8d24-c81cc0a912c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Head TFLite: /content/drive/MyDrive/DiabeticProject/hybrid_models/EfficientNetV2B0_fp16_FE_HEAD_from_KNN_fused_fp32.tflite | 0.006 MB\n",
            "üíæ Head TFLite: /content/drive/MyDrive/DiabeticProject/hybrid_models/EfficientNetV2B0_fp16_FE_HEAD_from_SVM_RBF_fused_fp32.tflite | 0.006 MB\n",
            "üíæ Head TFLite: /content/drive/MyDrive/DiabeticProject/hybrid_models/EfficientNetV2B0_fp16_FE_HEAD_from_RandomForest_fused_fp32.tflite | 0.006 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           Head                                     BackboneTFLite  \\\n",
              "0           KNN  /content/drive/MyDrive/DiabeticProject/tflite/...   \n",
              "1       SVM_RBF  /content/drive/MyDrive/DiabeticProject/tflite/...   \n",
              "2  RandomForest  /content/drive/MyDrive/DiabeticProject/tflite/...   \n",
              "\n",
              "                                          HeadTFLite   N   avg_ms   std_ms  \\\n",
              "0  /content/drive/MyDrive/DiabeticProject/hybrid_...  10  235.305  106.849   \n",
              "1  /content/drive/MyDrive/DiabeticProject/hybrid_...  10   75.515   19.926   \n",
              "2  /content/drive/MyDrive/DiabeticProject/hybrid_...  10   86.424   26.184   \n",
              "\n",
              "   min_ms   max_ms  \n",
              "0  88.400  366.775  \n",
              "1  58.539  116.118  \n",
              "2  54.409  126.373  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01f52e46-c771-4d65-8f88-95cd254f0f5d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Head</th>\n",
              "      <th>BackboneTFLite</th>\n",
              "      <th>HeadTFLite</th>\n",
              "      <th>N</th>\n",
              "      <th>avg_ms</th>\n",
              "      <th>std_ms</th>\n",
              "      <th>min_ms</th>\n",
              "      <th>max_ms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KNN</td>\n",
              "      <td>/content/drive/MyDrive/DiabeticProject/tflite/...</td>\n",
              "      <td>/content/drive/MyDrive/DiabeticProject/hybrid_...</td>\n",
              "      <td>10</td>\n",
              "      <td>235.305</td>\n",
              "      <td>106.849</td>\n",
              "      <td>88.400</td>\n",
              "      <td>366.775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SVM_RBF</td>\n",
              "      <td>/content/drive/MyDrive/DiabeticProject/tflite/...</td>\n",
              "      <td>/content/drive/MyDrive/DiabeticProject/hybrid_...</td>\n",
              "      <td>10</td>\n",
              "      <td>75.515</td>\n",
              "      <td>19.926</td>\n",
              "      <td>58.539</td>\n",
              "      <td>116.118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>/content/drive/MyDrive/DiabeticProject/tflite/...</td>\n",
              "      <td>/content/drive/MyDrive/DiabeticProject/hybrid_...</td>\n",
              "      <td>10</td>\n",
              "      <td>86.424</td>\n",
              "      <td>26.184</td>\n",
              "      <td>54.409</td>\n",
              "      <td>126.373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01f52e46-c771-4d65-8f88-95cd254f0f5d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-01f52e46-c771-4d65-8f88-95cd254f0f5d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-01f52e46-c771-4d65-8f88-95cd254f0f5d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-22e19834-dda1-4efd-8eab-0c80bbd5bed7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-22e19834-dda1-4efd-8eab-0c80bbd5bed7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-22e19834-dda1-4efd-8eab-0c80bbd5bed7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_17ed5b45-7c79-4c56-8e43-57a8ea44edcb\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_timing')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_17ed5b45-7c79-4c56-8e43-57a8ea44edcb button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_timing');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_timing",
              "summary": "{\n  \"name\": \"df_timing\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Head\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"KNN\",\n          \"SVM_RBF\",\n          \"RandomForest\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BackboneTFLite\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"/content/drive/MyDrive/DiabeticProject/tflite/EfficientNetV2B0_model_fp16.tflite\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HeadTFLite\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"/content/drive/MyDrive/DiabeticProject/hybrid_models/EfficientNetV2B0_fp16_FE_HEAD_from_KNN_fused_fp32.tflite\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 10,\n        \"max\": 10,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 89.27243186075606,\n        \"min\": 75.515,\n        \"max\": 235.305,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          235.305\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 48.47957081424436,\n        \"min\": 19.926,\n        \"max\": 106.849,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          106.849\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.54779547547363,\n        \"min\": 54.409,\n        \"max\": 88.4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          88.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 141.84922645659134,\n        \"min\": 116.118,\n        \"max\": 366.775,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          366.775\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Saved head files & timing CSV in: /content/drive/MyDrive/DiabeticProject/hybrid_models\n",
            " - KNN: /content/drive/MyDrive/DiabeticProject/hybrid_models/EfficientNetV2B0_fp16_FE_HEAD_from_KNN_fused_fp32.tflite\n",
            " - SVM_RBF: /content/drive/MyDrive/DiabeticProject/hybrid_models/EfficientNetV2B0_fp16_FE_HEAD_from_SVM_RBF_fused_fp32.tflite\n",
            " - RandomForest: /content/drive/MyDrive/DiabeticProject/hybrid_models/EfficientNetV2B0_fp16_FE_HEAD_from_RandomForest_fused_fp32.tflite\n",
            " - timing: /content/drive/MyDrive/DiabeticProject/hybrid_models/EfficientNetV2B0_fp16_FE_ALL_heads_latency.csv\n"
          ]
        }
      ]
    }
  ]
}